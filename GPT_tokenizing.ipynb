{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMK6CDMfGZMi8f4D+nFJBLk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moondancer92/DoitBert/blob/main/GPT_tokenizing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-cP4O3JLc5B"
      },
      "outputs": [],
      "source": [
        "!pip install ratsnlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZelKeD8L4lJ",
        "outputId": "58448435-694f-4e12-f4e8-f694ee3d175f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer_gpt = GPT2Tokenizer.from_pretrained(\"/gdrive/My Drive/nlpbook/bbpe\")\n",
        "tokenizer_gpt.pad_token = \"[PAD]\"\n",
        "\n",
        "sentences = [\n",
        "    \"아 더빙.. 진짜 짜증나네요 목소리\",\n",
        "    \"흠.. 포스터보고 초딩 영화인 줄.... 오버연기조차 가볍지않구나\",\n",
        "]\n",
        "tokenized_sentences = [tokenizer_gpt.tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "for tokenized_sentence in tokenized_sentences:\n",
        "    print(tokenized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yR50wbWtMAMT",
        "outputId": "ecaec3c7-d7d9-49e0-aa28-0ae964d546a2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ìķĦ', 'ĠëįĶë¹Ļ', '..', 'Ġì§Ħì§ľ', 'Ġì§ľì¦ĿëĤĺ', 'ëĦ¤ìļĶ', 'Ġëª©ìĨĮë¦¬']\n",
            "['íĿł', '..', 'Ġíı¬ìĬ¤íĦ°', 'ë³´ê³ł', 'Ġì´ĪëĶ©', 'ĠìĺģíĻĶìĿ¸', 'Ġì¤Ħ', '....', 'Ġìĺ¤ë²Ħ', 'ìĹ°ê¸°', 'ì¡°ì°¨', 'Ġê°Ģë³į', 'ì§ĢìķĬ', 'êµ¬ëĤĺ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 실제 모델 입력값\n",
        "bath_inputs = tokenizer_gpt(\n",
        "    sentences,\n",
        "    max_length=12,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        ")\n",
        "\n",
        "print(bath_inputs[\"input_ids\"])\n",
        "print(bath_inputs[\"attention_mask\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-V0vAfeSJ4N",
        "outputId": "c438fb9d-4c36-4924-fcea-0f79768f8e44"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[334, 2338, 263, 581, 4055, 464, 3808, 0, 0, 0, 0, 0], [3693, 263, 2720, 758, 2883, 3076, 1165, 422, 4444, 875, 2960, 7292]]\n",
            "[[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
          ]
        }
      ]
    }
  ]
}